\section{Лекция номер 9}

\subsection{Про p-примарные подпространства}
\begin{theorem-non}
    Пусть $\A \in \End V$ и $\mu_{\A} = p^{n_1}_1 \cdot \dots \cdot p^{n_s}_s$, где $p_i$ -- различные унитарные неприводимые многочлены.
    Тогда
    \[ V = \bigoplus_{i = 1}^{s} W_{p_i} \]
    То есть любое пространство можно разложить в прямую сумму $p$-примарных.
\end{theorem-non}
\begin{proof}
    Для доказательства применим несколько раз предыдущую лемму:
    \[ V = \bigoplus_{i = 1}^{s} V_i, \quad V_i \text{--- $\A$---инвариантно } \]  
    \quad Также по лемме: $\mu_{\A |_{V_i}} | p^{n_i}_i$.

    \quad Минимальный аннулирующий многочлен вектора делит минимальный многочлен оператора, поэтому каждый $v \in V_i$ является $p_i$-примарным.
    То есть мы доказали, что $V_i \subset W_{p_i}$.

    \quad Теперь проверим, что $W_{p_i} \subset V_i$. 
    Для простоты будем проверять только для $i = s$, очевидно, что для других индексов все будет аналогично.
    Возьмём $w \in W_{p_s}$ и разложим в сумму векторов из $V = \bigoplus\limits_{i = 1}^{s} V_i$:
    \begin{gather*}
        w = v_1 + v_2 + \dots + v_s, \quad v_i \in V_i \\
        \underbrace{w - v_s}_{\text{$ \in W_{p_s} $}}  = v_1 + v_2 + \dots + v_{s - 1} \\
    \end{gather*}
    \quad Знаем, что $\forall j \in [1, s - 1] \;\; \mu_{\A, v_j} \mid p^{n_j}_j$.
    Т.к. $p_s$ неприводим и $\mu_{\A, w - v_s} \mid \mu_{\A \mid_{V_s}} \mid p_s^{n_s}$,
    получаем, что $\mu_{\A, w - v_s} = p_s^k$, где $k \leqslant n_s$.
    Очевидно, что произведение аннуляторов будет обнулять каждый из $v_j$, где $j=1..(s-1)$: $(\mu_{\A, v_1} \dots \mu_{\A, v_{s - 1}})(\A)(v_j) = 0$.
    Тогда это произведение будет аннулятором суммы: 
    \begin{gather*}
        (\mu_{\A, v_1} \dots \mu_{\A, v_{s - 1}})(\A)(\underbrace{v_1 + \dots + v_{s - 1}}_{\text{$ w - v_s $}} ) = 0 \\
        \Longrightarrow (\mu_{\A, v_1} \dots \mu_{\A, v_{s - 1}}) \, \vdots \, \mu_{\A, w - v_s} \Longrightarrow (p_1^{n_1} \dots p_{s-1}^{n_{s-1}}) \, \vdots \, p_s^{k} \\
        \Longrightarrow k = 0 \Longrightarrow w - v_s = 0 \Longrightarrow w \in V_s
    \end{gather*}
\end{proof}

Эта теорема очень важна, потому что она сводит изучение произвольных операторов к изучению операторов, у которых минимальный многочлен это степень какого-либо неприводимого многочлена.
Зафиксируем еще одну теорему, но уже без доказательства (оно довольно объемное).

\begin{theorem-non}
    Пусть $V$ это $p$-примарное пространство, тогда $\exists v_1, \dots, v_t \in V$, т.ч.
    \[ V = \bigoplus_{i = 1}^{t} L_{v_i} \]
\end{theorem-non} 

Скомбинировав эту теорему с предыдущей, получаем, что любое пространство с действующем на нем линейным оператором можно разложить в прямую сумму циклических подпространств.

Что это всё будет означать на матричном языке?
Когда мы раскладываем пространство в прямую сумму инвариантных, то матрица оператора становится блочно-диагональной.
Вспомним, что для оператора, ограниченного на циклическое подпространство, матрица имеет особый вид.
Это так называемая сопровождающая матрица: 
\[
    \left(\begin{array}{ccccc}
        0 & 0 & \dots & 0 & -\alpha_0 \\ 
        1 & 0 & \dots & 0 & -\alpha_1 \\ 
        0 & 1 & \dots & 0 & -\alpha_2 \\ 
        \vdots & \vdots & \vdots & \vdots & \vdots \\ 
        0 & 0 & 0 & 1 & -\alpha_{d - 1}
    \end{array}\right)    
\]
Обозначив ее как $C(\mu_{\A, v_i})$, получаем, что матрица оператора примет вид:
\[
  [ \A ]_{E} = \left(\begin{array}{ccc}
  C(\mu_{\A, v_1}) &  & 0 \\ 
   & \ddots &  \\ 
  0 &  & C(\mu_{\A, v_s})
  \end{array}\right)  
\]

\begin{follow}
    Пусть $\chi_{\A} = \pm p^{m_1}_1 \cdot \dots \cdot p^{m_s}_s$, где $p_i$ --- различные неприводимые.
    Тогда 
        \[ \mu_{\A} = p^{n_1}_1 \cdot \dots \cdot p^{n_s}_s, \text{ где } 1 \leqslant n_i \leqslant m_i \]
    То есть никакой множитель из разложения не пропадет.
    \begin{proof}
        Осталось на упражнение. Надо доказать.
    \end{proof}
\end{follow}

\vspace*{5mm}

Отдельно рассмотрим случай, когда неприводимые множители линейны, т.е. когда характеристический многочлен раскладывается на линейные множители.
Изучение этого не является экзотикой, поскольку в $\C$ это любой многочлен, а в $\R$ таких много.

\subsection{Жорданова нормальная форма}
Пусть характеристический многочлен раскладывается на линейные множители: $\chi_{\A} = \pm (x - \lambda_1)^{m_1}\dots(x - \lambda_t)^{m_t}$.
Из последнего следствия мы поняли, что все такие $(x - \lambda_i)$ будут содержаться и в разложении минимального многочлена.
Тогда по теореме, доказанной в начале, наше пространство разложится в прямую сумму слагаемых вида $W_{x - \lambda_i}$.

Упростим запись, введя следующее обозначение: $R_{\lambda} := W_{x - \lambda_i}$ -- корневое подпространство, принадлежащее $\lambda$.
По определению оно будет равно $\{ v \in V \, | \, (\A - \lambda \mathcal{E})^h(v) = 0, \text{ где } h \in \N \}$.
Заменив $W_{x - \lambda_i}$ на $R_{\lambda_i}$ получаем \[ V = \bigoplus_{i = 1}^{t} R_{\lambda_i} \]

Рассмотрим случай, когда какая-то $\lambda = 0$. 
Тогда $R_0 = \{ v \in V \, | \, \A^h(v) = 0, \text{ где } h \in \N \}$. 
Заметим, что мы можем выбрать универсальную степень $h$ для всех векторов.
Это следует из конечномерности пространства (достаточно взять базис, для каждого вектора найти степень, а потом взять максимум).
Получаем, что $\exists h : (\A \big|_{R_0})^h = 0$.
Это так называемый нильпотентный оператор.

\begin{conj}
    Нильпотентный оператор -- оператор, некоторая степень которого обращается в 0. 
\end{conj}

Вспомним, что жорданова матрица -- это блочно-диагональная матрица, у которой блоки являются жордановыми клетками.

\begin{theorem-non}
    Пусть $\A \in \End V$. Тогда эквивалентны: \begin{enumerate}
        \item Характеристический многочлен оператора $\A$ раскладывается на линейные множители.
        \item Существует базис $E$ пространства $V$, т.ч. $[\A]_E$ жорданова. 
    \end{enumerate}
    Такой базис $E$ называется жордановым, и в общем случае он не единственнен. 
    Его поиск называется приведением матрицы к жордановой форме.
\end{theorem-non}
\begin{proof} \quad 

    \begin{enumerate}
        \item Пусть $\A$ нильпотентный.
        Заметим, что тогда $t = 1$ и единственная $\lambda = 0$.
        Почему? Ну пусть это не так, и существует собственное значение $\lambda \neq 0$ и собственный вектор $v \neq 0$, принадлежащий ему.
        Тогда $\A^N(v) = \lambda^N$, но по нильпотентности это должно быть равно 0. Противоречие.
    
        Таким образом, $\chi_{\A} = \pm x^n$, где $n$ -- размерность пространства.
        По недоказанной теореме прошлого параграфа $V = \bigoplus\limits_{i=1}^s L_{v_i}$.
        Заметим, что минимальные аннуляторы этих $v_i$ будут иметь вид $x^{l_i}$, так как обязаны делить минимальный аннулятор оператора, а он обязан делить характеристический многочлен.
        Для каждого $v_i$ в стандартном циклическом базисе у нас будет своя сопровождающая матрица, но ее особенностью будет то, что все коэффиценты в последнем столбце будут равны 0, так как они по определению являются коэффицентами минимального аннулятора:
        \[
            \left(\begin{array}{ccccc}
            0 & 0 & \dots & 0 & -\alpha_0 = 0 \\ 
            1 & 0 & \dots & 0 & -\alpha_1 = 0 \\ 
            0 & 1 & \dots & 0 & -\alpha_2 = 0 \\ 
            \vdots & \vdots & \vdots & \vdots & \vdots \\ 
            0 & 0 & 0 & 1 & -\alpha_{l_i - 1} = 0
        \end{array}\right)    
        \]
        Легко видеть, что получившаяся матрица -- жорданова клетка $J_{l_i}(0)$. 
        Объединив такие клетки по всем $v_i$, получим жорданову матрицу. 
        А тогда жорданов базис -- это сконкатенированные стандартные циклические базисы.

        \item Просто $t = 1$, т.е. $\chi_{\A} = \pm (x - \lambda)^n$. 
        По теореме Гамильтона-Кэли характеристический многочлен оператора это его аннулятор, поэтому $(\A - \lambda \mathcal{E})^n = 0$. 
        Получаем, что оператор $\B := \A - \lambda \mathcal{E}$ нильпотентный, а значит по первому пункту существует базис $E$, т.ч. в нем матрица $\B$ жорданова. 
        Матрица оператора $\A$ в том же базисе получается прибавление скаляра на главной диагонали, что тоже дает жорданову матрицу.
        
        \item Общий случай. Разложим в прямую сумму: $V = \bigoplus\limits_{i = 1}^{r} R_{\lambda_i}$.
        Заметим, что у $\A \big|_{R_{\lambda_i}}$ единственным собственным значением будет $\lambda_i$.
        Это верно, потому что собственные векторы $\lambda_i$ очевидно лежат в $R_{\lambda_i}$, а $R_{\lambda_i} \cap R_{\lambda_j} = \varnothing$ по определению прямой суммы, значит других собственных векторов там нет.
        Согласно второму пункту у $\A \big|_{R_{\lambda_i}}$ есть жорданов базис.
        Так как сумма у нас прямая, мы можем просто объединить все эти базисы и получить уже искомый.
    \end{enumerate}
\end{proof}

\notice Как уже упоминалось, жорданов базис не единственнен, но вот жорданова матрица определена однозначно с точностью до перестановки жордановых клеток. 

\vspace*{5mm}

Помимо жордановой формы существуют еще и другие, которые применимы уже к произвольным операторам.

В предыдущем параграфе мы поняли, что матрица любого оператора в нужном базисе примет вид:
\[
  [ \A ]_{E} = \left(\begin{array}{ccc}
  C(\mu_{\A, v_1}) &  & 0 \\ 
   & \ddots &  \\ 
  0 &  & C(\mu_{\A, v_t})
  \end{array}\right)  
\]
Пусть мы хотим сократить количество клеток в этой матрице, т.е разложить пространство в минимальную прямую сумму инвариантных подпространств.
Тогда, если выполняется условие этой леммы, мы можем это сделать:
\begin{lemma}
    Если $(\mu_{\A, v_1}, \mu_{\A, v_2}) = 1$ (т.е. они взаимно просты), то $L_{v_1} \otimes L_{v_2} = L_{v_1 + v_2}$.
\end{lemma}
\begin{proof}
    Упражнение. Надо бы доказать.
\end{proof}

Применяя эту лемму много раз, мы придем к так называемой Фробениусовой нормальной форме.
Именно она характеризуется минимальностью разложения в прямую сумму (то есть $t$ тут минимально):
    \[
  [ \A ]_{E} = \left(\begin{array}{ccc}
  C(f_1) &  & 0 \\ 
   & \ddots &  \\ 
  0 &  & C(f_t)
  \end{array}\right)  
\]
Тут $f_i$ уже не будут неприводимыми, но будут обладать таким свойством: $f_1 | f_2, f_2 | f_3, \dots, f_{t - 1} | f_t$.
Заметим, что такое разложение каноническое (однозначное), так как свойство делимости $(i + 1)$-го многочлена на $i$-тый запрещает нам переставлять клетки.

Приведем еще один вид матрицы произвольного оператора.
Пусть мы разложили наше пространство в прямую сумму инвариантных, т.е. матрица оператора является блочно-диагональной: \begin{gather*}
    \begin{pmatrix}
        D_1 &  & 0 \\ 
        & \ddots &  \\ 
        0 &  & D_m
    \end{pmatrix}
\end{gather*}
Тогда опять же, если правильно подобрать базис, каждый блок $D_i$ будет иметь вид: 
$$
\left[
    \begin{array}{c;{2pt/2pt}c;{2pt/2pt}c;{2pt/2pt}c;{2pt/2pt}cc}
        C(p) & 0 & \dots & \dots & 0 \\ \hdashline[2pt/2pt]
        1 & C(p) & \dots & \dots & 0 \\ \hdashline[2pt/2pt]
        0 & 1 & \ddots & 0 & 0 \\ \hdashline[2pt/2pt]
        0 & 0 & \dots & C(p) & 0 \\ \hdashline[2pt/2pt]
        0 & 0 & \dots & 1 & C(p)
    \end{array}
\right]
$$
Тут $C(p)$ это блок, являющийся сопровождающей матрицей многочлена $p$.
Важно, что единицы стоят не на всей диагонали, а только в местах состыковки блоков.

В некотором роде это обобщение жордановой нормальной формы для произвольного оператора.
Действительно, если $p = (x - \lambda)$, то каждое $C(p)$ это просто $\lambda$, и весь блок $D(i)$ превращается в жорданову клетку с собственным значением $\lambda$.


\subsection{Билинейные формы.}

\begin{conj}
Билинейная форма на $V$, где $V$ это линейное пространство над полем $K$, это отображение $\B: V \times V \to K$, линейное по двум аргументам:
\begin{itemize}
    \item $\B(\alpha_1 v_1 + \alpha_2 v_2, w) = \alpha_1 \B(v_1, w) + \alpha_2\B(v_2, w)$
    \item $\B(v, \alpha_1 w_1 + \alpha_2 w_2) = \alpha_1 \B(v, w_1) + \alpha_2\B(v, w_2)$
\end{itemize}
\end{conj}

\vspace*{5mm}
То есть мы фиксируем какой-то из аргументов и получаем линейный функционал на $V$.
Линейный функционал -- линейное отображение в поле $K$.

\underline{Примеры:}
\begin{enumerate}
    \item Стандартное скалярное произведение на $V = K^n$:

    \[ \B \left(
       \left(\begin{array}{c}
       \alpha_1 \\ 
       \vdots \\ 
       \alpha_n
       \end{array}\right),
       \left(\begin{array}{c}
       \beta_1 \\ 
       \vdots \\
       \beta_n
       \end{array}\right) 
    \right) = \alpha_1 \beta_1 + \dots + \alpha_n \beta_n \]

    \item Пусть $V = C[0, 1]$ -- непрерывные функции на отрезке $[0, 1]$. 
    
    Тогда $\B(f, g) = \int_{0}^{1} fg$ -- билинейная форма, так как интеграл линеен.

    \item Пусть $V = K^2$. \\
    Тогда следующее отображение является билейной формой: \[ \B \left(
        \left(\begin{array}{c}
        \alpha_1 \\ 
        \alpha_2
        \end{array}\right),
        \left(\begin{array}{c}
        \beta_1 \\ 
        \beta_2
        \end{array}\right)
    \right) = \alpha_1 \beta_2 - \alpha_2 \beta_1 \]
    По смыслу это определитель составленной из этих столбцов матрицы.
\end{enumerate}

\vspace*{5mm}

Пусть $V$ -- конечномерное, $E = (e_1, \dots, e_n)$ -- его базис.
Тогда билинейную форму, заданную на произвольных векторах, можно выразить через базисные: 
\[ \B(\alpha_1 e_1 + \dots + \alpha_n e_n, \beta_1 e_1 + \dots + \beta_n e_n) = 
\sum_{i=1}^{n} \sum_{j = 1}^{n} \alpha_i \beta_j \B(e_i, e_j) \]

Отсюда логичным оборазом вытекает следующее определение.

\begin{conj} Матрица Грама для билиненой формы $\B$ в базисе $E$ это матрица вида
\[ [\B]_E = \left(\begin{array}{ccc}
\B(e_1, e_1) & \dots & \B(e_1, e_n) \\ 
\dots & \ddots & \dots \\ 
\B(e_n, e_1) & \dots & \B(e_n, e_n)
\end{array}\right) \] 
\end{conj}

\vspace*{5mm}

Поймем, как с помощью матрицы Грама быстро посчитать $\B(\alpha_1 e_1 + \dots + \alpha_n e_n, \beta_1 e_1 + \dots + \beta_n e_n)$. 
Введем обозначение для столбцов координат: $X = \begin{pmatrix}
    \alpha_1 \\
    \vdots \\
    \alpha_n
\end{pmatrix}, Y = \begin{pmatrix}
    \beta_1 \\
    \vdots \\
    \beta_n
\end{pmatrix}$. 
Тогда верна следующая формула: \begin{gather*}
    B(EX, EY) = X^TBY \\
\end{gather*}
Она напрямую следует из матричных соображений: \begin{gather*}
    X^TB = (\alpha_1 B_{11} + \dots + \alpha_n B_{n1}, \dots, \alpha_1 B_{1n} + \dots + \alpha_n B_{nn}) \\
    X^TBY = \alpha_1 B_{11}\beta_1 + \dots + \alpha_n B_{n1}\beta_1 + \dots + \alpha_1 B_{1n}\beta_n + \dots + \alpha_n B_{nn}\beta_n 
\end{gather*}

\vspace*{5mm}

Чему равна матрица Грама в наших примерах?
Если в первом примере взять стандратный базис, то матрица Грама очевидно будет единичной.
Во втором примере у нас пространство бесконечномерное, поэтому никакой матрицы Грама нет. 
Если в третьем примере взять базис $E = \left(\begin{pmatrix}
    1 \\ 0
\end{pmatrix}, \begin{pmatrix}
    0 \\ 1
\end{pmatrix} \right)$, то матрица Грама будет равна $[\B]_E = \begin{pmatrix}
    0 & 1 \\
    -1 & 0
\end{pmatrix}$.

\vspace*{5mm}

Давайте посмотрим на то, как будет меняться матрица Грама при замене базиса.

\begin{theorem}(Матрица Грама при замене базиса)
    Пусть $E' = EC$, где $C \in GL(n, K)$.
    Тогда \[ [\B]_{E'} = C^T[\B]_EC \]
\end{theorem}
\begin{proof}
    Знаем, что $E' = EC$ и $X = CX'$, где $X$ -- координаты вектора.
    С одной стороны: \[ \B(EX, EY) = X^T[\B]_EY = (X')^TC^T[\B]_ECY' \]
    \quad С другой стороны: \[ \B(EX, EY) = \B(ECX', ECY') = B(E'X', E'Y') = (X')^T[\B]_{E'}Y' \]
    \quad Получаем, что для любых векторов $X'$ и $Y'$ выполняется равенство $(X')^TC^T[\B]_ECY' = (X')^T[\B]_{E'}Y'$.
    Отсюда уже следует, что $[\B]_{E'} = C^T[\B]_EC$.
    Почему?
    Возьмем за $X'$ $i$-тый базисный вектор, а за $Y'$ -- $j$-тый.
    Тогда умножение на $(X')^T$ и $Y'$ будет соответствовать вырезанию из матрицы элемента на позиции $(j, i)$.
    Выбирая различные $i$ и $j$ мы добьемся того, чтобы матрицы совпали.
\end{proof}

\vspace*{5mm}

\begin{conj}
    Рангом билинейной формы на конечномерном пространстве $V$ является $\rk{[\B]_E}$ для произвольного базиса $E$.
    Обозначается как $\rk{\B}$.
\end{conj}

\notice Это инвариант $\B$, так как при умножении на обратимую ранг не меняется.

\begin{conj}
    Если $\forall v, w \in V: \, \B(v, w) = \B(w, v)$, то 
    $\B$ -- симметрическая билинейная форма.
\end{conj}

\begin{conj}
    Если $\forall v, w \in V: \, \B(v, w) = -\B(w, v)$, то 
    $\B$ -- кососимметрическая билинейная форма.
\end{conj}

\begin{conj}
    Пусть $V$ -- линейное пространство над полем $K$ характеристики отличной от 2.
    Тогда отображение $q: V \to K$ называется квадратичной формой,
    если существует симмитрическая билинейная форма $\B$ на $V$, т.ч.
    \[ \forall v \in V: q(v) = \B(v, v) \]
\end{conj}

Можно заметить, что квадратичную форму задает однородный (все мономы имеют одинаковую степень) многочлен второй степени от координат вектора.
Действительно, пусть $E = (e_1, \dots, e_n)$ -- базис.
Тогда $q(x_1e_1 + \dots + x_ne_n) = X^TBX$.

Приведем пример для $n = 2$: \begin{gather*}
    B = \left(\begin{array}{cc}
        b_{11} & b_{12} \\ 
        b_{12} & b_{22}
        \end{array}\right) \Rightarrow X^TBX = b_{11}x^2_1 + 2b_{12}x_1x_2 + b_{22}x^2_2
\end{gather*}

Билинейную форму можно восстановить по квадратичной (помним, что $char K \neq 2$):
$$
    \B(v, w) = \frac{1}{2}(
        \underbrace{q(v + w)}_{\text{$\B(v + w, v + w)$}} - q(v) - q(w)
        )
$$
Говорят, что $\B$ является поляризацией $q$.

Таким образом, мы получили, что симметрическая билинейная и квадратичная формы тесно связаны и фактически являются эквивалентными понятиями.